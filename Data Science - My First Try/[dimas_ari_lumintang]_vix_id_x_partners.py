# -*- coding: utf-8 -*-
"""[Dimas Ari Lumintang]_VIX_ID/X Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hRTvAFO8BqZ0X7Zy5K4a1VFsQHQFBPS4
"""

import re
import joblib
import sklearn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from dython.nominal import associations
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_selector as selector
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix

"""**LOAD DATASET**"""

#read data
data = pd.read_csv('./data/loan_data_2007_2014.csv')
df = pd.DataFrame(data)

#read data description
data_desc = pd.read_excel("./data/LCDataDictionary.xlsx", sheet_name="browseNotes")

"""**ABOUT DATA**"""

#read the first 5 rows
df.head()

#show data description
data_desc[["Variable","Description"]]

df.info()

"""**EXPLORATORY DATA ANALYSIS (EDA)**"""

title_font = dict(size=20, weight="bold")

def plot_sum(df, y, title, **sns_kwargs):
    value_counts = df[y].value_counts()
    percentage = value_counts / value_counts.sum()
    percentage = percentage.apply("{:.3%}".format)

    plt.figure(figsize=(14, 10))
    plt.title(title, fontdict=title_font)
    sns.countplot(data=df, y=y, order=value_counts.index, **sns_kwargs)
    plt.ylabel("")
    plt.show()

    print(percentage)


def plot_dist(df, x, title, **sns_kwargs):
    plt.figure(figsize=(14, 10))
    plt.title(title, fontdict=title_font)
    sns.histplot(data=df, x=x, kde=True, **sns_kwargs)
    plt.ylabel("")
    plt.show()


def plot_box(df, x, y, title, **sns_kwargs):
    plt.figure(figsize=(14, 10))
    plt.title(title, fontdict=title_font)
    sns.boxplot(data=df, x=x, y=y, **sns_kwargs)
    plt.ylabel("")
    plt.show()
    plt.show()

"""*   **LOAN STATUS**

From the picture above, it can be seen that the most loan statuses are "Current" status, with the ratio of successful "Fully Paid" loans more than those that are not (late). This means that the loan status shows a positive trend.
"""

plot_sum(df, title="Loan Status",  y="loan_status")

#decide which category to use

approved = ["Fully Paid"]
reject = [
    "Charged Off",
    "Default",
    "Does not meet the credit policy. Status:Fully Paid",
    "Does not meet the credit policy. Status:Charged Off"
]

#replace with new label
def label_loan_status(value):
    if value in approved:
        return 1
    return 0

label_loan_status("Fully Paid")

#filter and apply function
filter_df = df[df["loan_status"].isin(approved + reject)].copy()
filter_df["loan_status"] = filter_df["loan_status"].apply(label_loan_status)

plot_sum(filter_df, y="loan_status", title="Loan Status")

"""*   **Correlation between variable**"""

#calculate the correlation of each variable
correlations = (filter_df.select_dtypes(exclude=object)
                         .corr()
                         .dropna(how="all", axis=1)
                         .dropna(how="all", axis=0)
)

correlations["loan_status"].abs().sort_values(ascending=True)

#filter the correlation between vmin - vmax
vmax, vmin = 0.99, 0.1

unstack_corr = correlations.unstack()
pstv_corr = (unstack_corr > vmin) & (unstack_corr < vmax)
ngtv_corr = (unstack_corr > -vmax) & (unstack_corr < -vmin)
high_corr = unstack_corr[pstv_corr | ngtv_corr]

trimmed_corr = high_corr.sort_values(ascending=True).unstack()

#create a mask to form the lower triangular matrix
mask = np.zeros_like(trimmed_corr)
mask[np.triu_indices_from(mask)] = False

"""
Heatmap to see several variables that have an influence on loan status"""

#show heatmap
plt.figure(figsize=(20, 20))
plot = sns.heatmap(
    trimmed_corr, 
    annot=True, 
    mask=mask,
    fmt=".2f", 
    cmap="viridis", 
    annot_kws={"size": 14})

plot.set_xticklabels(plot.get_xticklabels(), size=18)
plot.set_yticklabels(plot.get_yticklabels(), size=18)
plt.show()

affect_loan = high_corr.loc["loan_status"].abs().sort_values(ascending=True)
affect_loan

"""Identify correlated features with a limit value of 0.9"""

threshold = 0.9
affect_coll = (high_corr.abs()
                        .loc[high_corr > threshold]
                        .loc[affect_loan.index, affect_loan.index]
                        .sort_values(ascending=True)
)
affect_coll

left_index = affect_collision.index.get_level_values(0)
right_index = affect_collision.index.get_level_values(1)

def remove_collide_index(left_index, right_index):
    include, exclude = [], []

    for left, right in zip(left_index, right_index):
        if left not in include and left not in exclude:
            include.append(left)
        if right not in include and right not in exclude:
            exclude.append(right)
        
    return include, exclude


include_affect_col, exclude_affect_col = remove_collide_index(left_index, right_index)
include_affect_col, exclude_affect_col

affect_num_cols = affect_loan[~affect_loan.index.isin(exclude_affect_col)].index.to_list()
affect_num_cols

plot_dist(df=filter_df, x="total_rec_prncp", hue="loan_status", title="Principal received to date")

plot_dist(df=filter_df, x="recoveries", hue="loan_status", title="post charge off gross recovery")

x, y = "loan_status", "loan_amnt"
plot_box(df=filter_df, x=x, y=y, title="Total Loan Distribution")
filter_df.groupby(x)[y].describe()

x, y = "loan_status", "total_pymnt"
plot_box(df=filter_df, x=x, y=y, title="Distribution of Total Payments Received")
filter_df.groupby(x)[y].describe()

plot_sum(filter_df, y="purpose", title="Loan Purpose")

x, y = "int_rate", "grade"
order = filter_df[y].sort_values().unique()
plot_box(filter_df, x=x, y=y, title="Loan Rate", order=order)
plot_sum(df=filter_df, y=y, title="")
filter_df.groupby(y)[x].describe()

y = "home_ownership"
order = filter_df[y].sort_values().unique()
plot_sum(df=filter_df, y=y, title="")

"""**PREPROCESSING DATA**"""

#detailed information about data columns and rows
data_stat = pd.DataFrame()
data_stat.index = filter_df.columns
data_stat["unique_value"] = filter_df.nunique()
data_stat["missing_rate"] = filter_df.isna().mean()
data_stat["dtype"] = filter_df.dtypes
data_stat

#column where all data is missing
miss_col = data_stat[data_stat["missing_rate"] == 1].index.to_list()
print("Column where all data is missing: ")
print(miss_col)
print()

#columns with multiple categories
cat_col_stat = data_stat[data_stat["dtype"] == "object"]
vari_cat_col = cat_col_stat[cat_col_stat["unique_value"] > 1000].index.to_list()
print("Columns with multiple categories: ")
print(vari_cat_col)
print()

#column consisting of one value
single_valued_col = data_stat[data_stat["unique_value"] == 1].index.to_list()
print("Column consisting of one value: ")
print(single_valued_col)
print()

#unique column
vari_col = data_stat[data_stat["unique_value"] == filter_df.shape[0]].index.to_list()
print("Unique column: ")
print(vari_col)
print()

removed_features = miss_col + vari_col + vari_cat_col + single_valued_col

#eliminate unused features
pre_df = filter_df.loc[:, ~filter_df.columns.isin(removed_features)].copy()
pre_df.shape

"""**CATEGORICAL FEATURES**"""

#columns with categorical data
cat_features = pre_df.select_dtypes(include=object).columns
cat_features

date_cols = ["issue_d", "earliest_cr_line", "last_pymnt_d", "last_credit_pull_d", "next_pymnt_d"]

for col in date_cols:
    print(pre_df[col].value_counts().iloc[:5])
    print()

#correlation between dates and loan status
used_cols = date_cols + ["loan_status"] 
complete_correlation = associations(
    pre_df[used_cols], 
    filename='Date Correlation.png',
    figsize=(10,10)
)

#date feature we will use
affect_date_cols = ["issue_d", "last_pymnt_d", "last_credit_pull_d", "next_pymnt_d"]
affect_date_cols

#Remove date features that do not have a strong correlation with loan status
unused_cols = ["earliest_cr_line"]
pre_df = pre_df.drop(columns=unused_cols, errors="ignore")
pre_df.head()

other_cat_cols = cat_features[~cat_features.isin(date_cols)]
other_cat_cols

pre_df.loc[:, other_cat_cols].head()

unused_cols = ["desc", "zip_code", "sub_grade", "title"]
pre_df = pre_df.drop(columns=unused_cols, errors="ignore")
pre_df.head()

other_cat_cols = cat_features[~cat_features.isin(date_cols + unused_cols)]
other_cat_cols

#correlation between categorical features and loan status
used_cols = other_cat_cols.to_list() + ["loan_status"]
complete_correlation = associations(
    pre_df[used_cols], 
    filename='Cat Correlation.png',
    figsize=(10,10)
)

#categorical features we will use
affect_cat_cols = ["grade", "term"]
affect_cat_cols

#remove less influential features
used_cols = ["emp_title", "grade", "term"]
unused_cols = other_cat_cols[~other_cat_cols.isin(used_cols)]
pre_df = pre_df.drop(columns=unused_cols, errors="ignore")
pre_df.head()

"""**FEATURES THATE CORRELATED WITH TARGET**"""

#the columns we will use
predictor_cols = affect_num_cols + affect_cat_cols + affect_date_cols
predictor_cols

"""**IMPUTATION MISSING VALUE**"""

pre_df[predictor_cols].isna().mean().sort_values(ascending=True)

#fill data with "no"
pre_df["next_pymnt_d"] = pre_df["next_pymnt_d"].fillna("no")
top_next_pyment_d = pre_df["next_pymnt_d"].value_counts().head()

pre_df["last_pymnt_d"] = pre_df["last_pymnt_d"].fillna("no")
pre_df["last_credit_pull_d"] = pre_df["last_credit_pull_d"].fillna("no")

#fill data with "mode" value
mode = pre_df["inq_last_6mths"].mode().values[0]
pre_df["inq_last_6mths"] = pre_df["inq_last_6mths"].fillna(mode)

#check again if there is still missing data
pre_df[predictor_cols].isna().mean().sort_values(ascending=True)

"""**MODELING**"""

label = pre_df["loan_status"].copy()
features = pre_df[predictor_cols].copy()

print("Label shape:")
print(label.shape)

print("Features shape:")
print(features.shape)

num_features = features.select_dtypes(exclude="object")
cat_features = features.select_dtypes(include="object")

#numerical feature normalization
num_features = (num_features - num_features.mean()) / num_features.std()
num_features

#OneHotEncode categorical feature
cat_features = pd.get_dummies(cat_features)
cat_features

#merge features
features_full = pd.concat([num_features, cat_features], axis=1)
features_full.shape

"""Split Data"""

X_train, X_test, y_train, y_test = train_test_split(features_full, label, test_size=0.2, random_state=42, stratify=label)
X_train.shape, y_train.shape

logres = LogisticRegression(max_iter=500, solver="sag", class_weight="balanced", n_jobs=-1)
logres

logres.fit(X_train, y_train) #ERROR

"""**SAVING MODEL**"""

joblib.dump(logres, "logres.z")

logres = joblib.load("logres.z")

"""**MODEL EVALUATION**"""

test_label_counts = y_test.value_counts()
test_label_counts

test_label_counts.max() / test_label_counts.sum()

logres.score(X_train, y_train)

report = classification_report(y_true=y_train, y_pred=logres.predict(X_train))
print(report) #ERROR

logres.score(X_test, y_test) #ERROR

report = classification_report(y_true=y_test, y_pred=logres.predict(X_test))
print(report) #ERROR

conf = confusion_matrix(y_true=y_test, y_pred=logres.predict(X_test)) #ERROR

plt.figure(figsize=(10, 10))
sns.heatmap(conf, annot=True, fmt="g")
plt.show() #ERROR